{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62809968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 00:21:57.747012: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-20 00:21:57.747030: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-20 00:21:57.747044: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (rodrigo-ThinkPad-X390): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats.stats import pearsonr \n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "251bacb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pt_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "375f2c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entailment</th>\n",
       "      <th>id</th>\n",
       "      <th>similarity</th>\n",
       "      <th>t</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Relembre-se que o atleta estava afastado dos r...</td>\n",
       "      <td>André Gomes entra em campo quatro meses depois...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>O Sporting perdeu hoje com uma equipa muito ma...</td>\n",
       "      <td>O Sporting volta a perder com uma equipa russa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Cédric cruza para Tiago, que não chega a tempo.</td>\n",
       "      <td>Para os seus lugares, Cédric, Bruno Alves, Coe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Dessa forma, Turan só poderia ser inscrito em ...</td>\n",
       "      <td>Até Aleix Vidal e Arda Turan serem inscritos o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Os pupilos orientados por Andrés Madrid empata...</td>\n",
       "      <td>A equipa orientada por Andrés Madrid ainda não...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entailment  id  similarity  \\\n",
       "0       None   1         3.5   \n",
       "1       None   2         3.5   \n",
       "2       None   3         2.5   \n",
       "3       None   4         1.5   \n",
       "4       None   5         3.0   \n",
       "\n",
       "                                                   t  \\\n",
       "0  Relembre-se que o atleta estava afastado dos r...   \n",
       "1  O Sporting perdeu hoje com uma equipa muito ma...   \n",
       "2    Cédric cruza para Tiago, que não chega a tempo.   \n",
       "3  Dessa forma, Turan só poderia ser inscrito em ...   \n",
       "4  Os pupilos orientados por Andrés Madrid empata...   \n",
       "\n",
       "                                                   h  \n",
       "0  André Gomes entra em campo quatro meses depois...  \n",
       "1  O Sporting volta a perder com uma equipa russa...  \n",
       "2  Para os seus lugares, Cédric, Bruno Alves, Coe...  \n",
       "3  Até Aleix Vidal e Arda Turan serem inscritos o...  \n",
       "4  A equipa orientada por Andrés Madrid ainda não...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assin1_train = pd.read_xml(\"./assin/assin-ptpt-train.xml\")\n",
    "arr_train = np.array(assin1_train)\n",
    "assin1_val = pd.read_xml(\"./assin/assin-ptpt-dev.xml\")\n",
    "assin1_test = pd.read_xml(\"./assin/assin-ptpt-test.xml\")\n",
    "\n",
    "assin1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623381c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assin2_train = pd.read_xml(\"./assin2/assin2-train-only.xml\")\n",
    "assin2_test = pd.read_xml(\"./assin2/assin2-test.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8c894d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>h</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uma criança risonha está segurando uma pistola...</td>\n",
       "      <td>Uma criança está segurando uma pistola de água</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Os homens estão cuidadosamente colocando as ma...</td>\n",
       "      <td>Os homens estão colocando bagagens dentro do p...</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Uma pessoa tem cabelo loiro e esvoaçante e est...</td>\n",
       "      <td>Um guitarrista tem cabelo loiro e esvoaçante</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Batatas estão sendo fatiadas por um homem</td>\n",
       "      <td>O homem está fatiando a batata</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Um caminhão está descendo rapidamente um morro</td>\n",
       "      <td>Um caminhão está rapidamente descendo o morro</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   t  \\\n",
       "0  Uma criança risonha está segurando uma pistola...   \n",
       "1  Os homens estão cuidadosamente colocando as ma...   \n",
       "2  Uma pessoa tem cabelo loiro e esvoaçante e est...   \n",
       "3          Batatas estão sendo fatiadas por um homem   \n",
       "4     Um caminhão está descendo rapidamente um morro   \n",
       "\n",
       "                                                   h  similarity  \n",
       "0     Uma criança está segurando uma pistola de água         4.5  \n",
       "1  Os homens estão colocando bagagens dentro do p...         4.5  \n",
       "2       Um guitarrista tem cabelo loiro e esvoaçante         4.7  \n",
       "3                     O homem está fatiando a batata         4.7  \n",
       "4      Um caminhão está rapidamente descendo o morro         4.9  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_assin1_train = assin1_train[[\"t\", \"h\", \"similarity\"]]\n",
    "sentences_assin1_test = assin1_test[[\"t\", \"h\", \"similarity\"]]\n",
    "\n",
    "sentences_assin2_train = assin2_train[[\"t\", \"h\", \"similarity\"]]\n",
    "sentences_assin2_test = assin2_test[[\"t\", \"h\", \"similarity\"]]\n",
    "\n",
    "sentences_assin2_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a8e3d",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9d7a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    '''\n",
    "    Tokenize the given sentence in Portuguese.\n",
    "    :param text: text to be tokenized, as a string\n",
    "    '''\n",
    "    tokenizer_regexp = r'''(?ux)\n",
    "    # the order of the patterns is important!!\n",
    "    (?:[^\\W\\d_]\\.)+|                  # one letter abbreviations, e.g. E.U.A.\n",
    "    \\d+(?:[.,]\\d+)*(?:[.,]\\d+)|       # numbers in format 999.999.999,99999\n",
    "    \\w+(?:\\.(?!\\.|$))?|               # words with numbers (including hours as 12h30),\n",
    "                                      # followed by a single dot but not at the end of sentence\n",
    "    \\d+(?:[-\\\\/]\\d+)*|                # dates. 12/03/2012 12-03-2012\n",
    "    \\$|                               # currency sign\n",
    "    -+|                               # any sequence of dashes\n",
    "    \\S                                # any non-space character\n",
    "    '''\n",
    "    tokenizer = RegexpTokenizer(tokenizer_regexp)\n",
    "\n",
    "    return tokenizer.tokenize(sentence)\n",
    "\n",
    "\n",
    "def words_in_common(sentence1, sentence2):\n",
    "    '''\n",
    "    Return the proportion of words in common in a pair.\n",
    "    Repeated words are ignored.\n",
    "    '''\n",
    "    tokenset1 = set(tokenize_sentence(sentence1))\n",
    "    tokenset2 = set(tokenize_sentence(sentence2))\n",
    "\n",
    "    num_common_tokens = len(tokenset2.intersection(tokenset1))\n",
    "    proportion1 = num_common_tokens / len(tokenset1)\n",
    "    proportion2 = num_common_tokens / len(tokenset2)\n",
    "\n",
    "    return (proportion1+proportion2)/2\n",
    "\n",
    "\n",
    "def jaccard(sentence1, sentence2):\n",
    "    \"\"\"\n",
    "    returns the jaccard similarity between the two sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenset1 = set(tokenize_sentence(sentence1))\n",
    "    tokenset2 = set(tokenize_sentence(sentence2))\n",
    "    \n",
    "    num_common_tokens = len(tokenset2.intersection(tokenset1))\n",
    "    num_different_tokens = len(tokenset2.union(tokenset1))\n",
    "    \n",
    "    return num_common_tokens/num_different_tokens\n",
    "\n",
    "\n",
    "def vectorize(sentence1, sentence2, vectorizer):\n",
    "    \"\"\"\n",
    "    returns sentence 1 and 2 vectorized\n",
    "    \"\"\"\n",
    "    \n",
    "    if vectorizer == \"tfid\":\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    else:\n",
    "        vectorizer = CountVectorizer()\n",
    "    \n",
    "    vectorizer.fit([sentence1, sentence2])\n",
    "    \n",
    "    vec_sent1 = vectorizer.transform([sentence1])\n",
    "    vec_sent2 = vectorizer.transform([sentence2])\n",
    "    \n",
    "    return vec_sent1, vec_sent2\n",
    "    \n",
    "\n",
    "def cos_similarity(sentence1, sentence2, vectorizer):\n",
    "    \"\"\"\n",
    "    returns the cosine similarity between the two sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    if vectorizer not in [\"tfid\", \"count\"]:\n",
    "        return \"invalid vectorizer\"\n",
    "    \n",
    "    vec_sent1, vec_sent2 = vectorize(sentence1, sentence2, vectorizer)\n",
    "    \n",
    "    return cosine_similarity(vec_sent1, vec_sent2)[0][0]\n",
    "  \n",
    "    \n",
    "def diff_length(sentence1, sentence2):\n",
    "    \n",
    "    tokenset1 = tokenize_sentence(sentence1)\n",
    "    tokenset2 = tokenize_sentence(sentence2)\n",
    "    \n",
    "    return abs(len(tokenset1)-len(tokenset2))\n",
    "\n",
    "\n",
    "def diff_type(sentence1, sentence2, type_):\n",
    "    \n",
    "    if type_ not in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]:\n",
    "        return \"invalid word type\"\n",
    "    \n",
    "    doc2 = nlp(sentence2)\n",
    "    doc1 = nlp(sentence1)\n",
    "    \n",
    "    rest1 = [token for token in doc1 if token.pos_ == type_]\n",
    "    rest2 = [token for token in doc2 if token.pos_ == type_]\n",
    "    \n",
    "    return abs(len(rest1)-len(rest2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84a71f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sentence1, sentence2):\n",
    "    '''\n",
    "    Extract a vector of features from the given pairs and return\n",
    "    them as a numpy array.\n",
    "    '''\n",
    "    \n",
    "    overlap = words_in_common(sentence1, sentence2)\n",
    "    jaccard_similarity = jaccard(sentence1, sentence2)\n",
    "    cos_similarity_tfid = cos_similarity(sentence1, sentence2, \"tfid\")\n",
    "    cos_similarity_count = cos_similarity(sentence1, sentence2, \"count\")\n",
    "    diff_len = diff_length(sentence1, sentence2)\n",
    "    diff_nouns = diff_type(sentence1, sentence2, \"NOUN\")\n",
    "    diff_verbs = diff_type(sentence1, sentence2, \"VERB\")\n",
    "    diff_adjs = diff_type(sentence1, sentence2, \"ADJ\")\n",
    "    diff_advs = diff_type(sentence1, sentence2, \"ADV\")\n",
    "\n",
    "    return [overlap, jaccard_similarity, cos_similarity_tfid, cos_similarity_count, diff_len, diff_nouns, diff_verbs, diff_adjs, diff_advs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7120157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatized(sentence1, sentence2):\n",
    "    \n",
    "    doc1 = nlp(sentence1)\n",
    "    doc2 = nlp(sentence2)\n",
    "\n",
    "    lemma1 = [token.lemma_ for token in doc1]\n",
    "    lemma2 = [token.lemma_ for token in doc2]\n",
    "    \n",
    "    sent1 = \" \".join(lemma1)\n",
    "    sent2 = \" \".join(lemma2)\n",
    "\n",
    "    return sent1, sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d879391",
   "metadata": {},
   "outputs": [],
   "source": [
    "assin1_train_array = np.array(sentences_assin1_train)\n",
    "assin1_test_array = np.array(sentences_assin1_test)\n",
    "\n",
    "assin2_train_array = np.array(sentences_assin2_train)\n",
    "assin2_test_array = np.array(sentences_assin2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06be84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data_array):\n",
    "    overlaps = []\n",
    "    jaccards = []\n",
    "    cos_tfids = []\n",
    "    cos_counts = []\n",
    "    diff_lens = []\n",
    "    diff_nouns = []\n",
    "    diff_verbs = []\n",
    "    diff_adjs = []\n",
    "    diff_advs = []\n",
    "\n",
    "    for i in range(data_array.shape[0]):\n",
    "\n",
    "        line = data_array[i, :]\n",
    "\n",
    "        features = extract_features(line[0], line[1])\n",
    "\n",
    "        overlaps.append(features[0])\n",
    "        jaccards.append(features[1])\n",
    "        cos_tfids.append(features[2])\n",
    "        cos_counts.append(features[3])\n",
    "        diff_lens.append(features[4])\n",
    "        diff_nouns.append(features[5])\n",
    "        diff_verbs.append(features[6])\n",
    "        diff_adjs.append(features[7])\n",
    "        diff_advs.append(features[8])\n",
    "    \n",
    "    return pd.DataFrame(zip(overlaps, jaccards, cos_tfids, cos_counts, diff_lens, diff_nouns, diff_verbs, diff_adjs, diff_advs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46297df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_assin1 = get_features(assin1_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d33390b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_assin2 = get_features(assin2_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9f8de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_assin1 = get_features(assin1_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d12a2769",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test_assin2 = get_features(assin2_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62dfd5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass lists to dataframe\n",
    "\n",
    "assin1_train_Y = assin1_train_array[:, 2]\n",
    "assin1_test_Y = sentences_assin1_test[\"similarity\"]\n",
    "assin2_train_Y = assin2_train_array[:, 2]\n",
    "assin2_test_Y = sentences_assin2_test[\"similarity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6baa1f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sentence(sentence):\n",
    "            \n",
    "    doc = []\n",
    "    \n",
    "    for token in nlp(sentence):\n",
    "        if token.pos_ != \"NOUN\":\n",
    "            doc.append(token.lemma_)\n",
    "        else:\n",
    "            doc.append(str(token))\n",
    "            \n",
    "    new_sentence = \" \".join(doc)\n",
    "    \n",
    "    return new_sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94accb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformed_dataset(dataset):\n",
    "    \n",
    "    new_sent_1 = []\n",
    "    new_sent_2 = []\n",
    "    \n",
    "    for line in range(dataset.shape[0]):\n",
    "        \n",
    "        sentence1 = dataset[line, 0]\n",
    "        sentence2 = dataset[line, 1]\n",
    "        \n",
    "        transf_1 = transform_sentence(sentence1)\n",
    "        transf_2 = transform_sentence(sentence2)\n",
    "        \n",
    "        new_sent_1.append(transf_1)\n",
    "        new_sent_2.append(transf_2)\n",
    "        \n",
    "    new_dataset = pd.DataFrame(zip(new_sent_1, new_sent_2, dataset[:, 2]))\n",
    "    \n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeffd5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relembrer se que o atleta estar afastado de o ...</td>\n",
       "      <td>andré gomes entrar em campo quatro meses depoi...</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o sporting perder hoje com um equipa muito mai...</td>\n",
       "      <td>o sporting voltar a perder com um equipa russo...</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cédric cruzar para tiago , que não chegar a te...</td>\n",
       "      <td>para o seu lugares , cédric , bruno alves , co...</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dessa forma , turan só poder ser inscrito em j...</td>\n",
       "      <td>até aleix vidal e arda turan ser inscritos o b...</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o pupilos orientar por andrés madrid empatar o...</td>\n",
       "      <td>o equipa orientar por andrés madrid ainda não ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  relembrer se que o atleta estar afastado de o ...   \n",
       "1  o sporting perder hoje com um equipa muito mai...   \n",
       "2  cédric cruzar para tiago , que não chegar a te...   \n",
       "3  dessa forma , turan só poder ser inscrito em j...   \n",
       "4  o pupilos orientar por andrés madrid empatar o...   \n",
       "\n",
       "                                                   1    2  \n",
       "0  andré gomes entrar em campo quatro meses depoi...  3.5  \n",
       "1  o sporting voltar a perder com um equipa russo...  3.5  \n",
       "2  para o seu lugares , cédric , bruno alves , co...  2.5  \n",
       "3  até aleix vidal e arda turan ser inscritos o b...  1.5  \n",
       "4  o equipa orientar por andrés madrid ainda não ...  3.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assin1_train_new_dataset = get_transformed_dataset(assin1_train_array)\n",
    "assin1_test_new_dataset = get_transformed_dataset(assin1_test_array)\n",
    "\n",
    "assin1_train_new_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "980d878a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>um criança risonho estar segurar um pistola de...</td>\n",
       "      <td>um criança estar segurar um pistola de água</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o homens estar cuidadosamente colocar o malas ...</td>\n",
       "      <td>o homens estar colocar bagagens dentro de o po...</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>um pessoa ter cabelo loiro e esvoaçante e esta...</td>\n",
       "      <td>um guitarrista ter cabelo loiro e esvoaçante</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>batatas estar ser fatiar por um homem</td>\n",
       "      <td>o homem estar fatiar o batata</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>um caminhão estar descer rapidamente um morro</td>\n",
       "      <td>um caminhão estar rapidamente descer o morro</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  um criança risonho estar segurar um pistola de...   \n",
       "1  o homens estar cuidadosamente colocar o malas ...   \n",
       "2  um pessoa ter cabelo loiro e esvoaçante e esta...   \n",
       "3              batatas estar ser fatiar por um homem   \n",
       "4      um caminhão estar descer rapidamente um morro   \n",
       "\n",
       "                                                   1    2  \n",
       "0        um criança estar segurar um pistola de água  4.5  \n",
       "1  o homens estar colocar bagagens dentro de o po...  4.5  \n",
       "2       um guitarrista ter cabelo loiro e esvoaçante  4.7  \n",
       "3                      o homem estar fatiar o batata  4.7  \n",
       "4       um caminhão estar rapidamente descer o morro  4.9  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assin2_train_new_dataset = get_transformed_dataset(assin2_train_array)\n",
    "assin2_test_new_dataset = get_transformed_dataset(assin2_test_array)\n",
    "\n",
    "assin2_train_new_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86844ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "assin1_train_new_array = np.array(assin1_train_new_dataset)\n",
    "assin1_test_new_array = np.array(assin1_test_new_dataset)\n",
    "\n",
    "assin2_train_new_array = np.array(assin2_train_new_dataset)\n",
    "assin2_test_new_array = np.array(assin2_test_new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5f7a48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_train_assin1 = get_features(assin1_train_new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6f5854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_train_assin2 = get_features(assin2_train_new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97b12864",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_test_assin1 = get_features(assin1_test_new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d0b616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_test_assin2 = get_features(assin2_test_new_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a22e6",
   "metadata": {},
   "source": [
    "# Rule Based experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db57e76",
   "metadata": {},
   "source": [
    "### ASSIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0d48c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_corrs_assin1 = []\n",
    "cosine_count_assin1 = []\n",
    "cosine_tfid_assin1 = []\n",
    "\n",
    "jaccard_corrs_assin1_lemma = []\n",
    "cosine_count_assin1_lemma = []\n",
    "cosine_tfid_assin1_lemma = []\n",
    "\n",
    "for line in range(assin1_test_array.shape[0]):\n",
    "    \n",
    "    sentence1 = assin1_test_array[line, 0] \n",
    "    sentence2 = assin1_test_array[line, 1]\n",
    "    \n",
    "    # Rules without lemmatization\n",
    "    \n",
    "    jaccard_corr = jaccard(sentence1, sentence2)\n",
    "    jaccard_corrs_assin1.append(jaccard_corr)\n",
    "    \n",
    "    cosine_sim_count = cos_similarity(sentence1, sentence2, \"count\")\n",
    "    cosine_count_assin1.append(cosine_sim_count)\n",
    "    \n",
    "    cosine_sim_tfid = cos_similarity(sentence1, sentence2, \"tfid\")\n",
    "    cosine_tfid_assin1.append(cosine_sim_tfid)\n",
    "    \n",
    "    # Rules with lemmatization\n",
    "    \n",
    "    lemma1, lemma2 = lemmatized(sentence1, sentence2)\n",
    "    \n",
    "    jaccard_corr = jaccard(lemma1, lemma2)\n",
    "    jaccard_corrs_assin1_lemma.append(jaccard_corr)\n",
    "    \n",
    "    cosine_sim_count = cos_similarity(lemma1, lemma2, \"count\")\n",
    "    cosine_count_assin1_lemma.append(cosine_sim_count)\n",
    "    \n",
    "    cosine_sim_tfid = cos_similarity(lemma1, lemma2, \"tfid\")\n",
    "    cosine_tfid_assin1_lemma.append(cosine_sim_tfid)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "cca3a788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without lemmatization:\n",
      "\n",
      "Jaccard: 0.62046011950866\n",
      "Cosine - count: 0.6188696520627122\n",
      "Cosine - tfid: 0.6148674000978605\n",
      "\n",
      "With lemmatization:\n",
      "\n",
      "Jaccard: 0.6238416856989887\n",
      "Cosine - count: 0.5646118276823116\n",
      "Cosine - tfid: 0.5682784475870755\n"
     ]
    }
   ],
   "source": [
    "pearson_corr_jaccard = pearsonr(jaccard_corrs_assin1, assin1_test_Y) \n",
    "pearson_corr_cosine_count = pearsonr(cosine_count_assin1, assin1_test_Y) \n",
    "pearson_corr_cosine_tfid = pearsonr(cosine_tfid_assin1, assin1_test_Y) \n",
    "\n",
    "pearson_corr_jaccard_lemma = pearsonr(jaccard_corrs_assin1_lemma, assin1_test_Y) \n",
    "pearson_corr_cosine_count_lemma = pearsonr(cosine_count_assin1_lemma, assin1_test_Y) \n",
    "pearson_corr_cosine_tfid_lemma = pearsonr(cosine_tfid_assin1_lemma, assin1_test_Y) \n",
    "\n",
    "print(f\"Without lemmatization:\\n\\nJaccard: {pearson_corr_jaccard[0]}\\nCosine - count: {pearson_corr_cosine_count[0]}\\nCosine - tfid: {pearson_corr_cosine_tfid[0]}\\n\")\n",
    "print(f\"With lemmatization:\\n\\nJaccard: {pearson_corr_jaccard_lemma[0]}\\nCosine - count: {pearson_corr_cosine_count_lemma[0]}\\nCosine - tfid: {pearson_corr_cosine_tfid_lemma[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb23129",
   "metadata": {},
   "source": [
    "### ASSIN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4eb2b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_corrs_assin2 = []\n",
    "cosine_count_assin2 = []\n",
    "cosine_tfid_assin2 = []\n",
    "\n",
    "jaccard_corrs_assin2_lemma = []\n",
    "cosine_count_assin2_lemma = []\n",
    "cosine_tfid_assin2_lemma = []\n",
    "\n",
    "\n",
    "for line in range(assin2_test_array.shape[0]):\n",
    "    \n",
    "    sentence1 = assin2_test_array[line, 0] \n",
    "    sentence2 = assin2_test_array[line, 1]\n",
    "    \n",
    "    # Rules without lemmatization\n",
    "    \n",
    "    jaccard_corr = jaccard(sentence1, sentence2)\n",
    "    jaccard_corrs_assin2.append(jaccard_corr)\n",
    "    \n",
    "    cosine_sim_count = cos_similarity(sentence1, sentence2, \"count\")\n",
    "    cosine_count_assin2.append(cosine_sim_count)\n",
    "    \n",
    "    cosine_sim_tfid = cos_similarity(sentence1, sentence2, \"tfid\")\n",
    "    cosine_tfid_assin2.append(cosine_sim_tfid)\n",
    "    \n",
    "    # Rules with lemmatization\n",
    "    \n",
    "    lemma1, lemma2 = lemmatized(sentence1, sentence2)\n",
    "    \n",
    "    jaccard_corr = jaccard(lemma1, lemma2)\n",
    "    jaccard_corrs_assin2_lemma.append(jaccard_corr)\n",
    "    \n",
    "    cosine_sim_count = cos_similarity(lemma1, lemma2, \"count\")\n",
    "    cosine_count_assin2_lemma.append(cosine_sim_count)\n",
    "    \n",
    "    cosine_sim_tfid = cos_similarity(lemma1, lemma2, \"tfid\")\n",
    "    cosine_tfid_assin2_lemma.append(cosine_sim_tfid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2c1af043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without lemmatization:\n",
      "\n",
      "Jaccard: 0.5168209326334956\n",
      "Cosine - count: 0.5668184076435708\n",
      "Cosine - tfid: 0.5527139063190045\n",
      "\n",
      "With lemmatization:\n",
      "\n",
      "Jaccard: 0.6050355532746834\n",
      "Cosine - count: 0.5778223586847617\n",
      "Cosine - tfid: 0.577571906394748\n"
     ]
    }
   ],
   "source": [
    "pearson_corr_jaccard = pearsonr(jaccard_corrs_assin2, assin2_test_Y) \n",
    "pearson_corr_cosine_count = pearsonr(cosine_count_assin2, assin2_test_Y) \n",
    "pearson_corr_cosine_tfid = pearsonr(cosine_tfid_assin2, assin2_test_Y) \n",
    "\n",
    "pearson_corr_jaccard_lemma = pearsonr(jaccard_corrs_assin2_lemma, assin2_test_Y) \n",
    "pearson_corr_cosine_count_lemma = pearsonr(cosine_count_assin2_lemma, assin2_test_Y) \n",
    "pearson_corr_cosine_tfid_lemma = pearsonr(cosine_tfid_assin2_lemma, assin2_test_Y) \n",
    "\n",
    "print(f\"Without lemmatization:\\n\\nJaccard: {pearson_corr_jaccard[0]}\\nCosine - count: {pearson_corr_cosine_count[0]}\\nCosine - tfid: {pearson_corr_cosine_tfid[0]}\\n\")\n",
    "print(f\"With lemmatization:\\n\\nJaccard: {pearson_corr_jaccard_lemma[0]}\\nCosine - count: {pearson_corr_cosine_count_lemma[0]}\\nCosine - tfid: {pearson_corr_cosine_tfid_lemma[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5754ea",
   "metadata": {},
   "source": [
    "# Machine Learning experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e037f52e",
   "metadata": {},
   "source": [
    "## Linear regression - ASSIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "eca6aa3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg_assin1 = LinearRegression()\n",
    "\n",
    "#lin_reg_assin1.fit(features_train_assin1, assin1_train_Y)\n",
    "lin_reg_assin1.fit(new_features_train_assin1, assin1_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d274d5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6345771696441544\n"
     ]
    }
   ],
   "source": [
    "pred_lin_reg_assin1 = lin_reg_assin1.predict(X=new_features_test_assin1)\n",
    "\n",
    "pearson_corr = pearsonr(pred_lin_reg_assin1, assin1_test_Y) \n",
    "\n",
    "print(pearson_corr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6c64cfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6761287280805739\n"
     ]
    }
   ],
   "source": [
    "pred_lin_reg_assin1 = lin_reg_assin1.predict(X=features_train_assin1)\n",
    "\n",
    "pearson_corr = pearsonr(pred_lin_reg_assin1, assin1_train_Y) \n",
    "\n",
    "print(pearson_corr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d9ca2f",
   "metadata": {},
   "source": [
    "## Linar regression - ASSIN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ceb9dc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg_assin2 = LinearRegression()\n",
    "\n",
    "#lin_reg_assin2.fit(features_train_assin2, assin2_train_Y)\n",
    "lin_reg_assin2.fit(new_features_train_assin2, assin2_train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e4147b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5570367806427622\n"
     ]
    }
   ],
   "source": [
    "pred_lin_reg_assin2 = lin_reg_assin2.predict(X=features_test_assin2)\n",
    "\n",
    "pearson_corr = pearsonr(pred_lin_reg_assin2, assin2_test_Y) \n",
    "\n",
    "print(pearson_corr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "16fc5e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5783150184611987\n"
     ]
    }
   ],
   "source": [
    "pred_lin_reg_assin2 = lin_reg_assin2.predict(X=features_train_assin2)\n",
    "\n",
    "pearson_corr = pearsonr(pred_lin_reg_assin2, assin2_train_Y) \n",
    "\n",
    "print(pearson_corr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0eead9",
   "metadata": {},
   "source": [
    "## BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2746858c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/rodrigo/.cache/torch/sentence_transformers/neuralmind_bert-base-portuguese-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/rodrigo/.cache/torch/sentence_transformers/neuralmind_bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No sentence-transformers model found with name /home/rodrigo/.cache/torch/sentence_transformers/neuralmind_bert-base-portuguese-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/rodrigo/.cache/torch/sentence_transformers/neuralmind_bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No sentence-transformers model found with name /home/rodrigo/.cache/torch/sentence_transformers/neuralmind_bert-base-portuguese-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/rodrigo/.cache/torch/sentence_transformers/neuralmind_bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No sentence-transformers model found with name /home/rodrigo/.cache/torch/sentence_transformers/neuralmind_bert-base-portuguese-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/rodrigo/.cache/torch/sentence_transformers/neuralmind_bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No sentence-transformers model found with name /home/rodrigo/.cache/torch/sentence_transformers/neuralmind_bert-base-portuguese-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/rodrigo/.cache/torch/sentence_transformers/neuralmind_bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No sentence-transformers model found with name /home/rodrigo/.cache/torch/sentence_transformers/neuralmind_bert-base-portuguese-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/rodrigo/.cache/torch/sentence_transformers/neuralmind_bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('neuralmind/bert-base-portuguese-cased')\n",
    "model2 = SentenceTransformer('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "model_MSE = SentenceTransformer('neuralmind/bert-base-portuguese-cased')\n",
    "model2_MSE = SentenceTransformer('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "model_MNL = SentenceTransformer('neuralmind/bert-base-portuguese-cased')\n",
    "model2_MNL = SentenceTransformer('neuralmind/bert-base-portuguese-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2405abb",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3369587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assin_sent1 = list(sentences_assin1_train['t'])\n",
    "#assin_sent2 = list(sentences_assin1_train['h'])\n",
    "#assin_sim = list(sentences_assin1_train['similarity'])\n",
    "\n",
    "assin_sent1 = list(assin1_train_new_dataset[0])\n",
    "assin_sent2 = list(assin1_train_new_dataset[1])\n",
    "assin_sim = list(assin1_train_new_dataset[2])\n",
    "\n",
    "assin1_input = []\n",
    "\n",
    "for i in range(len(assin_sent1)):\n",
    "    \n",
    "    s1 = assin_sent1[i]\n",
    "    s2 = assin_sent2[i]\n",
    "    sim = assin_sim[i]\n",
    "    \n",
    "    example = InputExample(texts=[s1, s2], label=sim)\n",
    "    \n",
    "    assin1_input.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a00127c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assin2_sent1 = list(sentences_assin2_train['t'])\n",
    "#assin2_sent2 = list(sentences_assin2_train['h'])\n",
    "#assin2_sim = list(sentences_assin2_train['similarity'])\n",
    "\n",
    "assin2_sent1 = list(assin2_train_new_dataset[0])\n",
    "assin2_sent2 = list(assin2_train_new_dataset[1])\n",
    "assin2_sim = list(assin2_train_new_dataset[2])\n",
    "\n",
    "assin2_input = []\n",
    "\n",
    "for i in range(len(assin_sent2)):\n",
    "    \n",
    "    s1 = assin2_sent1[i]\n",
    "    s2 = assin2_sent2[i]\n",
    "    sim = assin2_sim[i]\n",
    "    \n",
    "    example = InputExample(texts=[s1, s2], label=sim)\n",
    "    \n",
    "    assin2_input.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b6f5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader1 = DataLoader(assin1_input, shuffle=True, batch_size=16)\n",
    "train_dataloader2 = DataLoader(assin2_input, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237dfb5f",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1243d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assin1_sent1_test = list(sentences_assin1_test['t'])\n",
    "#assin1_sent2_test = list(sentences_assin1_test['h'])\n",
    "#assin1_sim_test = list(sentences_assin1_test['similarity'])\n",
    "\n",
    "assin1_sent1_test = list(assin1_test_new_dataset[0])\n",
    "assin1_sent2_test = list(assin1_test_new_dataset[1])\n",
    "assin1_sim_test = list(assin1_test_new_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df2710e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assin2_sent1_test = list(sentences_assin2_test['t'])\n",
    "#assin2_sent2_test = list(sentences_assin2_test['h'])\n",
    "#assin2_sim_test = list(sentences_assin2_test['similarity'])\n",
    "\n",
    "assin2_sent1_test = list(assin2_test_new_dataset[0])\n",
    "assin2_sent2_test = list(assin2_test_new_dataset[1])\n",
    "assin2_sim_test = list(assin2_test_new_dataset[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a5b62",
   "metadata": {},
   "source": [
    "### ASSIN1 model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b0b95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_cosine = losses.CosineSimilarityLoss(model)\n",
    "train_loss_contrastive = losses.ContrastiveLoss(model_MSE)\n",
    "train_loss_MNRL = losses.MultipleNegativesRankingLoss(model_MNL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8767da0",
   "metadata": {},
   "source": [
    "#### Cosine loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5218580f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5746452021bc492aa6a6d8a6b967c1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350c38daaf514523a788a0638b551242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.fit(train_objectives=[(train_dataloader1, train_loss_cosine)], epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371405df",
   "metadata": {},
   "source": [
    "#### Constrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1831354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d818eedca6477e818a95457546c76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f26b4ee19f48c2a95019ff0a948db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_MSE.fit(train_objectives=[(train_dataloader1, train_loss_contrastive)], epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4010492e",
   "metadata": {},
   "source": [
    "#### Multiple Negatives Ranking Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcf9bb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d987148c1a90457d881603e88519f52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e4933067da4ce5b476df15dd5dab39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_MNL.fit(train_objectives=[(train_dataloader1, train_loss_MNRL)], epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18077e31",
   "metadata": {},
   "source": [
    "### ASSIN2 model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab09de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_loss_cosine = losses.CosineSimilarityLoss(model2)\n",
    "train2_loss_contrastive = losses.ContrastiveLoss(model2_MSE)\n",
    "train2_loss_MNRL = losses.MultipleNegativesRankingLoss(model2_MNL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daaf0b2",
   "metadata": {},
   "source": [
    "#### Cosine Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f311c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda6a2ec78ca43b8b6e67e7c016178f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36b9ff94db846df8b569e1c8e53c28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2.fit(train_objectives=[(train_dataloader2, train2_loss_cosine)], epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a1530",
   "metadata": {},
   "source": [
    "#### Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e31a3c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdbb5e7bc9546e9bdacfda59511763b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6d7163f02348ce82e49bccdfad7a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2_MSE.fit(train_objectives=[(train_dataloader2, train2_loss_contrastive)], epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37623dbd",
   "metadata": {},
   "source": [
    "#### Multiple Negatives Ranking Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa46ad67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31e17f47d874c8192ca238691f002c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50957f14cd4475d9d3445ce38c994c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2_MNL.fit(train_objectives=[(train_dataloader2, train2_loss_MNRL)], epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5088df",
   "metadata": {},
   "source": [
    "### Results - ASSIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1047c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sims_bert(model, sentences1, sentences2): \n",
    "    \n",
    "    if len(sentences1) != len(sentences2):\n",
    "        \n",
    "        return \"Both sentence lists should have the same length!\"\n",
    "\n",
    "    sims = []\n",
    "\n",
    "    for i in range(len(sentences1)):\n",
    "\n",
    "        sent1 = sentences1[i]\n",
    "        sent2 = sentences2[i]\n",
    "\n",
    "        encode1 = model.encode(sent1)\n",
    "        encode2 = model.encode(sent2)\n",
    "\n",
    "        sim_encodes = cosine_similarity(encode1.reshape(1, -1), encode2.reshape(1,-1))[0][0]\n",
    "\n",
    "        sims.append(sim_encodes)\n",
    "    \n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "303d226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSINE LOSS\n",
    "cosines_loss_sims = get_sims_bert(model, assin1_sent1_test, assin1_sent2_test)\n",
    "\n",
    "# CONTRASTIVE LOSS\n",
    "contrastive_loss_sims = get_sims_bert(model_MSE, assin1_sent1_test, assin1_sent2_test)\n",
    "\n",
    "# MNR Loss\n",
    "MNR_loss_sims = get_sims_bert(model_MNL, assin1_sent1_test, assin1_sent2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "daa08a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine loss: 0.5254046174049791\n",
      "Contrastive Loss: 0.5252380132237491\n",
      "MNR Loss: 0.5406509495921523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pearson_cosine_loss = pearsonr(cosines_loss_sims, assin1_test_Y) \n",
    "pearson_contrastive_loss = pearsonr(contrastive_loss_sims, assin1_test_Y) \n",
    "pearson_MNR_loss = pearsonr(MNR_loss_sims, assin1_test_Y) \n",
    "\n",
    "print(f\"Cosine loss: {pearson_cosine_loss[0]}\\nContrastive Loss: {pearson_contrastive_loss[0]}\\nMNR Loss: {pearson_MNR_loss[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590fe4b",
   "metadata": {},
   "source": [
    "### Results - ASSIN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c888e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSINE LOSS\n",
    "cosines_loss_sims2 = get_sims_bert(model2, assin2_sent1_test, assin2_sent2_test)\n",
    "\n",
    "# CONTRASTIVE LOSS\n",
    "contrastive_loss_sims2 = get_sims_bert(model2_MSE, assin2_sent1_test, assin2_sent2_test)\n",
    "\n",
    "# MNR Loss\n",
    "MNR_loss_sims2 = get_sims_bert(model2_MNL, assin2_sent1_test, assin2_sent2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "348c9fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine loss: 0.6405380549880414\n",
      "Contrastive Loss: 0.6419603253698404\n",
      "MNR Loss: 0.6658863607004178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pearson_cosine_loss2 = pearsonr(cosines_loss_sims2, assin2_test_Y) \n",
    "pearson_contrastive_loss2 = pearsonr(contrastive_loss_sims2, assin2_test_Y) \n",
    "pearson_MNR_loss2 = pearsonr(MNR_loss_sims2, assin2_test_Y) \n",
    "\n",
    "print(f\"Cosine loss: {pearson_cosine_loss2[0]}\\nContrastive Loss: {pearson_contrastive_loss2[0]}\\nMNR Loss: {pearson_MNR_loss2[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f727b",
   "metadata": {},
   "source": [
    "# Saving the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc415693",
   "metadata": {},
   "source": [
    "As we can see, the model that achieved the best results was the Sentence Transformer model using MNR Loss.\n",
    "So, now we will save the model in order to be able to use it in other applications.\n",
    "\n",
    "We will also fine tune this model using data from both ASSIN and ASSIN2 in order to improve its capability of generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "654b7047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/rodrigo/.cache/torch/sentence_transformers/neuralmind_bert-base-portuguese-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/rodrigo/.cache/torch/sentence_transformers/neuralmind_bert-base-portuguese-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "best_model = SentenceTransformer('neuralmind/bert-base-portuguese-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d61ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = losses.MultipleNegativesRankingLoss(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ef353",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2c65d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent1 = list(sentences_assin1_train['t'])\n",
    "sent1_assin2 = list(sentences_assin2_train['t'])\n",
    "\n",
    "all_sent2 = list(sentences_assin1_train['h'])\n",
    "sent2_assin2 = list(sentences_assin2_train['h'])\n",
    "\n",
    "all_sim =  list(sentences_assin1_train['similarity'])\n",
    "sim_assin2 =  list(sentences_assin2_train['similarity'])\n",
    "\n",
    "all_sent1.extend(sent1_assin2)\n",
    "all_sent2.extend(sent2_assin2)\n",
    "all_sim.extend(sim_assin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "716effd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_input = []\n",
    "\n",
    "for i in range(len(all_sent1)):\n",
    "    \n",
    "    s1 = all_sent1[i]\n",
    "    s2 = all_sent2[i]\n",
    "    sim = all_sim[i]\n",
    "    \n",
    "    example = InputExample(texts=[s1, s2], label=sim)\n",
    "    \n",
    "    best_model_input.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bf8a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_dataloader = DataLoader(best_model_input, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "908b16a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51f7b1aa68a4d2aa684ec9818fadb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf04cddc5f84bde82d00b747ff316cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model.fit(train_objectives=[(best_model_dataloader, best_loss)], epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10c064e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120be3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
